{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto de Extracción de Datos de la API Open Weather y Creación de Tabla en Amazon Redshift por *Carlos Horta*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Introducción*\n",
    "\n",
    "Este primer entregable, de un total de tres, tiene como consigna el desarrollo de un script que permita la extracción de datos desde una API pública y, simultáneamente, la creación de la tabla correspondiente en Amazon RedShift. La finalidad de este proyecto es establecer un punto de partida, un script ETL inicial, que será fundamental en la realización del proyecto final. El script en cuestión deberá ser capaz de extraer datos en formato JSON y procesarlos en un diccionario de Python. Además, se espera la creación de una versión preliminar de la tabla donde los datos extraídos se cargarán posteriormente.\n",
    "\n",
    "## *API seleccionada*\n",
    "Para llevar a cabo este proyecto, se ha optado por utilizar la API de Open Weather, específicamente para obtener datos relacionados con el pronóstico del tiempo en la Ciudad de México durante los próximos cinco días, con intervalos de actualización cada tres horas. Del archivo JSON proporcionado por la API, se focalizará la atención en la extracción de información de los apartados 'main' y 'weather', excluyendo otros detalles como 'clouds', 'wind', 'visibility' y 'sys'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in g:\\mi unidad\\carlos\\coderhouse\\data engineering\\data engineering entregables\\entregable 1\\venv\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in g:\\mi unidad\\carlos\\coderhouse\\data engineering\\data engineering entregables\\entregable 1\\venv\\lib\\site-packages (from pandas) (1.26.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in g:\\mi unidad\\carlos\\coderhouse\\data engineering\\data engineering entregables\\entregable 1\\venv\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in g:\\mi unidad\\carlos\\coderhouse\\data engineering\\data engineering entregables\\entregable 1\\venv\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in g:\\mi unidad\\carlos\\coderhouse\\data engineering\\data engineering entregables\\entregable 1\\venv\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in g:\\mi unidad\\carlos\\coderhouse\\data engineering\\data engineering entregables\\entregable 1\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: requests in g:\\mi unidad\\carlos\\coderhouse\\data engineering\\data engineering entregables\\entregable 1\\venv\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in g:\\mi unidad\\carlos\\coderhouse\\data engineering\\data engineering entregables\\entregable 1\\venv\\lib\\site-packages (from requests) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in g:\\mi unidad\\carlos\\coderhouse\\data engineering\\data engineering entregables\\entregable 1\\venv\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in g:\\mi unidad\\carlos\\coderhouse\\data engineering\\data engineering entregables\\entregable 1\\venv\\lib\\site-packages (from requests) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in g:\\mi unidad\\carlos\\coderhouse\\data engineering\\data engineering entregables\\entregable 1\\venv\\lib\\site-packages (from requests) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: sqlalchemy in g:\\mi unidad\\carlos\\coderhouse\\data engineering\\data engineering entregables\\entregable 1\\venv\\lib\\site-packages (1.4.49)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in g:\\mi unidad\\carlos\\coderhouse\\data engineering\\data engineering entregables\\entregable 1\\venv\\lib\\site-packages (from sqlalchemy) (3.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: sqlalchemy-redshift in g:\\mi unidad\\carlos\\coderhouse\\data engineering\\data engineering entregables\\entregable 1\\venv\\lib\\site-packages (0.8.14)\n",
      "Requirement already satisfied: redshift_connector[full] in g:\\mi unidad\\carlos\\coderhouse\\data engineering\\data engineering entregables\\entregable 1\\venv\\lib\\site-packages (2.0.915)\n",
      "Requirement already satisfied: scramp<1.5.0,>=1.2.0 in g:\\mi unidad\\carlos\\coderhouse\\data engineering\\data engineering entregables\\entregable 1\\venv\\lib\\site-packages (from redshift_connector[full]) (1.4.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in g:\\mi unidad\\carlos\\coderhouse\\data engineering\\data engineering entregables\\entregable 1\\venv\\lib\\site-packages (from redshift_connector[full]) (2023.3.post1)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.7.0 in g:\\mi unidad\\carlos\\coderhouse\\data engineering\\data engineering entregables\\entregable 1\\venv\\lib\\site-packages (from redshift_connector[full]) (4.12.2)\n",
      "Requirement already satisfied: boto3<2.0.0,>=1.9.201 in g:\\mi unidad\\carlos\\coderhouse\\data engineering\\data engineering entregables\\entregable 1\\venv\\lib\\site-packages (from redshift_connector[full]) (1.28.72)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.23.0 in g:\\mi unidad\\carlos\\coderhouse\\data engineering\\data engineering entregables\\entregable 1\\venv\\lib\\site-packages (from redshift_connector[full]) (2.31.0)\n",
      "Requirement already satisfied: lxml>=4.6.5 in g:\\mi unidad\\carlos\\coderhouse\\data engineering\\data engineering entregables\\entregable 1\\venv\\lib\\site-packages (from redshift_connector[full]) (4.9.3)\n",
      "Requirement already satisfied: botocore<2.0.0,>=1.12.201 in g:\\mi unidad\\carlos\\coderhouse\\data engineering\\data engineering entregables\\entregable 1\\venv\\lib\\site-packages (from redshift_connector[full]) (1.31.72)\n",
      "Requirement already satisfied: packaging in g:\\mi unidad\\carlos\\coderhouse\\data engineering\\data engineering entregables\\entregable 1\\venv\\lib\\site-packages (from redshift_connector[full]) (23.2)\n",
      "Requirement already satisfied: setuptools in g:\\mi unidad\\carlos\\coderhouse\\data engineering\\data engineering entregables\\entregable 1\\venv\\lib\\site-packages (from redshift_connector[full]) (65.5.0)\n",
      "Requirement already satisfied: numpy in g:\\mi unidad\\carlos\\coderhouse\\data engineering\\data engineering entregables\\entregable 1\\venv\\lib\\site-packages (from redshift_connector[full]) (1.26.1)\n",
      "Requirement already satisfied: pandas in g:\\mi unidad\\carlos\\coderhouse\\data engineering\\data engineering entregables\\entregable 1\\venv\\lib\\site-packages (from redshift_connector[full]) (2.1.2)\n",
      "Requirement already satisfied: SQLAlchemy<2.0.0,>=0.9.2 in g:\\mi unidad\\carlos\\coderhouse\\data engineering\\data engineering entregables\\entregable 1\\venv\\lib\\site-packages (from sqlalchemy-redshift) (1.4.49)\n",
      "Requirement already satisfied: soupsieve>1.2 in g:\\mi unidad\\carlos\\coderhouse\\data engineering\\data engineering entregables\\entregable 1\\venv\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.7.0->redshift_connector[full]) (2.5)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in g:\\mi unidad\\carlos\\coderhouse\\data engineering\\data engineering entregables\\entregable 1\\venv\\lib\\site-packages (from boto3<2.0.0,>=1.9.201->redshift_connector[full]) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.8.0,>=0.7.0 in g:\\mi unidad\\carlos\\coderhouse\\data engineering\\data engineering entregables\\entregable 1\\venv\\lib\\site-packages (from boto3<2.0.0,>=1.9.201->redshift_connector[full]) (0.7.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in g:\\mi unidad\\carlos\\coderhouse\\data engineering\\data engineering entregables\\entregable 1\\venv\\lib\\site-packages (from botocore<2.0.0,>=1.12.201->redshift_connector[full]) (2.8.2)\n",
      "Requirement already satisfied: urllib3<2.1,>=1.25.4 in g:\\mi unidad\\carlos\\coderhouse\\data engineering\\data engineering entregables\\entregable 1\\venv\\lib\\site-packages (from botocore<2.0.0,>=1.12.201->redshift_connector[full]) (2.0.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in g:\\mi unidad\\carlos\\coderhouse\\data engineering\\data engineering entregables\\entregable 1\\venv\\lib\\site-packages (from requests<3.0.0,>=2.23.0->redshift_connector[full]) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in g:\\mi unidad\\carlos\\coderhouse\\data engineering\\data engineering entregables\\entregable 1\\venv\\lib\\site-packages (from requests<3.0.0,>=2.23.0->redshift_connector[full]) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in g:\\mi unidad\\carlos\\coderhouse\\data engineering\\data engineering entregables\\entregable 1\\venv\\lib\\site-packages (from requests<3.0.0,>=2.23.0->redshift_connector[full]) (2023.7.22)\n",
      "Requirement already satisfied: asn1crypto>=1.5.1 in g:\\mi unidad\\carlos\\coderhouse\\data engineering\\data engineering entregables\\entregable 1\\venv\\lib\\site-packages (from scramp<1.5.0,>=1.2.0->redshift_connector[full]) (1.5.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in g:\\mi unidad\\carlos\\coderhouse\\data engineering\\data engineering entregables\\entregable 1\\venv\\lib\\site-packages (from SQLAlchemy<2.0.0,>=0.9.2->sqlalchemy-redshift) (3.0.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in g:\\mi unidad\\carlos\\coderhouse\\data engineering\\data engineering entregables\\entregable 1\\venv\\lib\\site-packages (from pandas->redshift_connector[full]) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in g:\\mi unidad\\carlos\\coderhouse\\data engineering\\data engineering entregables\\entregable 1\\venv\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<2.0.0,>=1.12.201->redshift_connector[full]) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: psycopg2 in g:\\mi unidad\\carlos\\coderhouse\\data engineering\\data engineering entregables\\entregable 1\\venv\\lib\\site-packages (2.9.9)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Instalación de bibliotecas necesarias para el proyecto\n",
    "\n",
    "%pip install pandas\n",
    "%pip install requests\n",
    "%pip install sqlalchemy\n",
    "%pip install \"redshift_connector[full]\" sqlalchemy-redshift\n",
    "%pip install psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar las bibliotecas necesarias\n",
    "\n",
    "from configparser import ConfigParser\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import sqlalchemy as sa\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from sqlalchemy.exc import SQLAlchemyError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una instancia de ConfigParser\n",
    "config = ConfigParser()\n",
    "\n",
    "# Leer el archivo de configuración\n",
    "config_dir = \"config/config.ini\"\n",
    "config.read(config_dir)  \n",
    "\n",
    "# Acceder a la sección 'redshift'\n",
    "redshift_username = config['redshift']['username']\n",
    "redshift_password = config['redshift']['password']\n",
    "redshift_host = config['redshift']['host']\n",
    "redshift_port = config['redshift']['port']\n",
    "redshift_database = config['redshift']['database']\n",
    "\n",
    "# Acceder a la sección 'api_openweather'\n",
    "openweather_name = config['api_openweather']['name']\n",
    "openweather_key = config['api_openweather']['key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carlo\\AppData\\Local\\Temp\\ipykernel_43668\\4181528118.py:55: RemovedIn20Warning: Deprecated API features detected! These feature(s) are not compatible with SQLAlchemy 2.0. To prevent incompatible upgrades prior to updating applications, ensure requirements files are pinned to \"sqlalchemy<2.0\". Set environment variable SQLALCHEMY_WARN_20=1 to show all deprecation warnings.  Set environment variable SQLALCHEMY_SILENCE_UBER_WARNING=1 to silence this message. (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
      "  engine.execute(create_table_sql)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos cargados exitosamente en Amazon Redshift.\n"
     ]
    }
   ],
   "source": [
    "# Realizar la solicitud HTTP\n",
    "base_url = \"https://api.openweathermap.org/data/2.5/\"\n",
    "endpoint = f\"forecast?lat=19.42847&lon=-99.12766&units=metric&appid={openweather_key}\"\n",
    "url = f\"{base_url}{endpoint}\"\n",
    "resp = requests.get(url)\n",
    "\n",
    "# Verificar si la solicitud se realizó con éxito\n",
    "if resp.status_code == 200:\n",
    "    data = resp.json()  # Convertir la respuesta JSON en un diccionario de Python\n",
    "\n",
    "    if data:\n",
    "        pronosticos = data.get('list', [])\n",
    "\n",
    "        if pronosticos:\n",
    "            # Crear un DataFrame de Pandas con los datos de interés\n",
    "            df = pd.DataFrame(pronosticos)[['dt_txt', 'main', 'weather']]\n",
    "\n",
    "            # Transformar los datos JSON en las columnas deseadas\n",
    "            df['temp'] = df['main'].apply(lambda x: x['temp'])\n",
    "            df['feels_like'] = df['main'].apply(lambda x: x['feels_like'])\n",
    "            df['temp_min'] = df['main'].apply(lambda x: x['temp_min'])\n",
    "            df['temp_max'] = df['main'].apply(lambda x: x['temp_max'])\n",
    "            df['pressure'] = df['main'].apply(lambda x: x['pressure'])\n",
    "            df['sea_level'] = df['main'].apply(lambda x: x['sea_level'])\n",
    "            df['grnd_level'] = df['main'].apply(lambda x: x['grnd_level'])\n",
    "            df['humidity'] = df['main'].apply(lambda x: x['humidity'])\n",
    "\n",
    "            df['weather_id'] = df['weather'].apply(lambda x: x[0]['id'])\n",
    "            df['weather_main'] = df['weather'].apply(lambda x: x[0]['main'])\n",
    "            df['weather_description'] = df['weather'].apply(lambda x: x[0]['description'])\n",
    "\n",
    "            # Eliminar las columnas JSON que ya no son necesarias\n",
    "            df = df.drop(columns=['main', 'weather'])\n",
    "\n",
    "            # Configuración de la conexión a Amazon Redshift desde config.ini\n",
    "            engine = create_engine(f'redshift+psycopg2://{redshift_username}:{redshift_password}@{redshift_host}:{redshift_port}/{redshift_database}')\n",
    "\n",
    " # Definir la estructura de la tabla y crear la tabla en Amazon Redshift\n",
    "            table_name = 'weather_data'\n",
    "            create_table_sql = \"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS weather_data (\n",
    "                dt_txt TIMESTAMP PRIMARY KEY,\n",
    "                temp FLOAT,\n",
    "                feels_like FLOAT,\n",
    "                temp_min FLOAT,\n",
    "                temp_max FLOAT,\n",
    "                pressure INT,\n",
    "                sea_level INT,\n",
    "                grnd_level INT,\n",
    "                humidity INT,\n",
    "                weather_main VARCHAR(255),\n",
    "                weather_description VARCHAR(255)\n",
    "            ) DISTSTYLE EVEN;\n",
    "            \"\"\"\n",
    "            engine.execute(create_table_sql)\n",
    "\n",
    "            try:\n",
    "                # Cargar los datos en Amazon Redshift\n",
    "                df.to_sql(table_name, engine, if_exists='append', index=False)\n",
    "                print('Datos cargados exitosamente en Amazon Redshift.')\n",
    "            except SQLAlchemyError as e:\n",
    "                print(f'Error al cargar datos en Amazon Redshift: {str(e)}')\n",
    "        else:\n",
    "            print('No se encontraron pronósticos para la Ciudad de México en la respuesta de la API.')\n",
    "    else:\n",
    "        print('No se pudo obtener una respuesta válida de la API de OpenWeatherMap.')\n",
    "else:\n",
    "    print(f'Error al realizar la solicitud HTTP. Código de estado: {resp.status_code}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Conclusión*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este primer entregable del proyecto, hemos logrado desarrollar un script ETL inicial que nos permite extraer datos de una API pública y, al mismo tiempo, crear la estructura de la tabla correspondiente en Amazon RedShift. El objetivo de este ejercicio es sentar las bases para el proyecto final, donde se realizará un proceso ETL más completo y se explotarán los datos de manera más profunda.\n",
    "\n",
    "Hemos seleccionado la API de Open Weather para obtener datos precisos sobre el pronóstico del tiempo en la Ciudad de México durante los próximos cinco días, con actualizaciones cada tres horas. El script se enfoca en la extracción de información relevante de los apartados 'main' y 'weather' del archivo JSON proporcionado por la API, dejando de lado otros detalles que no son necesarios para este proyecto.\n",
    "\n",
    "A medida que avanzamos en el proyecto, este script inicial se convertirá en una parte esencial de nuestro flujo de trabajo de procesamiento de datos. La extracción y transformación de datos se volverán más sofisticadas, y la carga en la base de datos se optimizará para una explotación efectiva de la información. En resumen, este entregable marca el inicio de un proyecto emocionante en el que utilizaremos nuestras habilidades técnicas para obtener insights valiosos de los datos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
